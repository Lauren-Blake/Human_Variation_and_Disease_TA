\documentclass[12pt]{report}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{color}


\begin{document}
Name: \hrulefill
Date: \hrulefill

\begin{center}


\color{black}
\bigskip
\large{\textbf{Human Variation and Disease: Discussion 1 Notes and Concept Checks}}

\bigskip

\color{red} Answer Key

\color{black}

\bigskip
\normalsize{\textit{The goal of this discussion is to review the statistical concepts described in Appendix B described in Gillespie's ``Population Genetics: A Concise Guide'' that will be useful for this class. Questions to check conceptual understanding are provided, incorporating course material when applicable.}}

\bigskip
\normalsize{\textbf{\textit{Complete this worksheet to the best of your ability and bring it to the first discussion section. Also, please email me (leblake@uchicago.edu) any concepts or sections that you found challenging by Thursday night at midnight. }}}

\end{center}

\section{What are random variables?}

\begin{itemize}

\item A discrete random variable (e.g. \textbf{X}), is a function that takes on certain values depending on the outcome of some event, trial, or experiment.

\item An event with \textit{n} outcomes and its associated random variable may be described as follows:

\begin{tabular}{| l | c | r |}
	\hline
 Outcome & Value of \textbf{X} & Probability \\ \hline
  1 & \textit{$x_{1}$} & \textit{$p_{1}$} \\
  2 & \textit{$x_{2}$}  & \textit{$p_{2}$} \\
  3 & \textit{$x_{3}$} & \textit{$p_{3}$} \\
  . & . & . \\
  . & . & . \\
  \textit{i} & \textit{$x_{i}$}  & \textit{$p_{i}$} \\
  . & .  & . \\
  \textit{n} & \textit{$x_{n}$}  & \textit{$n_{i}$} \\ \hline
\end{tabular}

\bigskip

\item \textbf{Concept Check 1:} In the table above, what is \\ Prob \{{\textbf{X} = \textit{$x_{i}$} }\}? \color{red} \textit{$p_{i}$} 

\color{black}
\item \textbf{Concept Check 2:} In the table above, what is Prob \{{\textbf{X} = \textit{$x_{1}$} $\cup$ \textbf{X} = \textit{$x_{3}$}}\}? \color{red} \textit{$p_{1}$} + \textit{$p_{3}$}  

\end{itemize}

\section{Moments of Random Variables}

\subsection{Mean/Expectation}

\begin{itemize}

\item The mean is a weighted average of the values taken by the random variable.

\item It is defined as \textit{E}\{\textbf{X}\} =  $\sum\limits_{i = 1}\limits^{n}$ \textit{$p_{i}$}\textit{$x_{i}$} 

\item It is often denoted by $\mu$.

\item \textbf{Concept Check 3}: Population geneticists are often interested in the mean fitness of a population, $\bar{w}$. Here, the random variable takes on the fitnesses of genotypes and the probabilities are the frequencies of those genotypes. Using the table below, write the formula for the mean fitness of a population. (For now, don't worry about the biological meanings of \textit{h} and \textit{s}.) 

\bigskip
\begin{tabular}{| l | c | r |}
	\hline
 Outcome & Value of \textbf{X} & Probability \\ \hline
  \textit{$A_{1}$$A_{1}$} & 1 & \textit{$p^{2}$} \\
  \textit{$A_{1}$$A_{2}$} & 1 - \textit{hs} & 2\textit{pq} \\
  \textit{$A_{2}$$A_{2}$} & 1- \textit{s} & \textit{$q^{2}$} \\ \hline
\end{tabular}

\bigskip

\color{red} $\bar{w}$ = (1) \textit{$p^{2}$} + (1-\textit{hs}) 2\textit{pq} + (1-\textit{s}) \textit{$q^{2}$} = 1 - 2\textit{pqhs} - \textit{$q^{2}$}\textit{s}

\bigskip
\color{black}

\textit{Note: this question is based off of the material covered in the Human Variation and Disease Lecture titled ``Selection I''}

\item \textbf{Concept Check 4}: We have measured the beaks of 5 subpopulations of finches and recorded our findings in the table below. What is the expected value of beak size for the population of finches? (In other words, what is \textit{E}\{\textbf{X}\}? Assume only these subpopulations make up the population of finches that we are interested in and there is no variation in beak length within sub-populations.) 

\bigskip
\begin{tabular}{| l | c | r |}
	\hline
 Subpopulation Number & Beak length (mm) & Subpopulation size \\ \hline
  1 & 0.4 & 100 \\
  2 & 0.55 &  300 \\
  3 & 0.65 & 150 \\ 
  4 & 0.35 &  250 \\
  5 & 0.47 & 200 \\  \hline
\end{tabular}

\bigskip

\color{red} 0.4*0.1 + 0.55*.3 + 0.65*.15 + 0.35*.25 + 0.47*0.2 = 0.484 mm

\color{black}

\bigskip
\textit{Note: this question is based off of the material covered in the Human Variation and Disease Lecture titled ``Quantitative Genetics I''}

\end{itemize}

\subsection{Variance}

\begin{itemize}

\item The variance of a random variable is the expectation of the squared deviations from the mean.

\item It is defined by Var\{\textbf{X}\} =  \textit{E}\{$\big (\textbf{X - $\mu$}\big)^{2}$\} = \textit{E}\{$\textbf{X}^{2}$\} -  \textit{E}\{$\textbf{X}\}^{2}$

\item It is often denoted by $\sigma^{2}$.

\item \textbf{Concept Check 5}: Using the table from the previous Concept Check, what is variance in beak size for this population of finches? (Again, assume only these subpopulations make up the population of finches that we are interested in.)

\bigskip

\color{red} \textit{E}\{$\textbf{X}^{2}$\} = $0.4^2$ $\times$ 0.1 + $0.55^2$  $\times$ 0.3 + $0.65^2$  $\times$ 0.15 + $0.35^2$  $\times$ 0.25 + $0.47^2$  $\times$ 0.2 = 0.24493

\textit{E}\{$\textbf{X}\}^{2}$ = $(answer from above)^2$ = $0.484^2$ = 0.23426

Var\{\textbf{X}\} =  \textit{E}\{$\textbf{X}^{2}$\} -  \textit{E}\{$\textbf{X}\}^{2}$ = 0.24493 - 0.23426 = 0.01067

\color{black}
\bigskip
\textit{Note: this question is based off of the material covered in the Human Variation and Disease Lecture titled ``Quantitative Genetics I''}


\end{itemize}

\section{Noteworthy discrete random variables}

\subsection{Bernoulli random variable}

\begin{itemize}

\item The Bernoulli random variable is used when there is one trial of an event where there are only two possible outcomes: success (where \textbf{X} = 1) and failure (where \textbf{X} = 0). 

\item \textbf{Concept Check 6:}  The mean of a Bernoulli random variable is \color{red} 1 $\times$ \textit{p} + 0 $\times$ \textit{q}  = \textit{p}
\color{black}
and the variance is \color{red} \textit{pq}

\bigskip
\textit{E}\{$\textbf{X}^{2}$\} = $1^{2}$  $\times$ \textit{p} + $0^{2}$  $\times$ \textit{q} = \textit{p}
 
\textit{E}\{$\textbf{X}\}^{2}$ = $(mean)^{2}$ = $p^{2}$

Var =  \textit{E}\{$\textbf{X}^{2}$\} -  \textit{E}\{$\textbf{X}\}^{2}$ = \textit{p} - $p^2$ = \textit{p(1-p)} = \textit{pq}



\color{black}
\item \textbf{Concept Check 7}: What is a use of the Bernoulli distribution in genetics? In your example, describe what is a ``success'' and what is a '``failure''.  

\end{itemize}
\bigskip

\color{red} One example is the proportion of haploid chromosomes carrying an allele for a particular Mendelian disease. A ``success'' is the chromosome carrying the disease allele and ``failure'' is the chromosome not carrying the disease allele. The probability of success is the frequency of the Mendelian disease-causing variant in the population and the probability of failure is the complement of the probability of success. 

\color{black}
\subsection{Binomial Random Variable}

\begin{itemize}

\item These random variables represent the number of success in \textit{n} independent trials when the probability of success for any one trial is \textit{p}. 

\item The random variable can take o the values 0, 1, $\ldots$, \textit{n} with probabilities

Prob \{\textbf{X} = \textit{i}\} = ${n \choose x}$ \textit{$p^{i}$} $\big( 1- \textit{p} \big)^{\textit{n-i}}$

\item \textbf{Concept Check 8}: How are Bernoulli and Binomial random variables related?

\color{red} The binomial distribution allows for n number of independent Bernoulli trials.

\color{black}

\item \textbf{Concept Check 9}: Keeping in mind your answer to Concept Check 2, what is the mean of a binomial random variable? The variance?

\color{red} Multiply the mean and then the variance of the Bernoulli distribution by n so that the mean is \textit{n} and the variance is \textit{npq}. 

\color{black}

\item \textbf{Challenge 1}: In the Wright-Fisher model that we will study in class, we want to know the proportion of offspring that carry allele A from some population. What factors will be important for us to consider given that the Wright-Fisher model is a binomial sampling distribution?

\color{red} Population size to find the number of gametes (2N) and the number of adult chromosomes that carry allele \textit{A} will allow us to find the probability of j gametes with allele \textit{A} in the offspring given i gametes with allele \textit{A} in the adult population. Note: there are several assumptions that allow us to model this as a binomial distribution.


\color{black}

\end{itemize}

\subsection{Poisson Random Variable}

\begin{itemize}

\item Poisson Random Variables are obtained by taking the limit of binomial random variables as \textit{n} $\rightarrow$ $\infty$ and \textit{p} $\rightarrow$ 0. 

\item Poisson random variables can take values 0, 1, $\ldots$, $\infty$ with probabilities 

Prob \{\textbf{X} = \textit{i}\} = $\frac{e^{-\mu}\mu^{i}}{i!}$

\item \textbf{Concept Check 10:} When can the Poisson distribution be used instead of the Binomial distribution? 

\color{red} When p (probability of a success) is much smaller than n (number of trials\/sample size).  

\color{black}
\bigskip
\item \textbf{Concept Check 11:} How does the Poisson distribution look different than the Binomial distribution? 

\color{red} 
The Poisson distribution will have a higher density closer to 0 than the Binomial distribution. We can see that in the plot below.  

\color{black}
\bigskip

Overlay an example of each of the distributions on a plot below. Each distribution should have $\mu$ = 0.05 and \textit{n}= 10:

\color{red} To create this plot, I used the following commands: \\
plot( dpois( x=0:9, lambda=0.05 ), pch=1, main = "Density plots of Poisson and Binomial distributions", xlab = "Number of successes", ylab = "Probability") \\
points( dbinom(x=0:9, 10, 0.05), pch = 15) \\
leg.txt <- c("white circles = Poisson", "black squares = Binomial") \\
legend('topright', leg.txt, lty = 1, col = c('white', 'white')) \\
\color{black}



\includegraphics[width =1\textwidth]{/Users/laurenblake/Dropbox/Human_Variation_and_Disease_TA/Distributions_plot.png}

\bigskip


\item \textbf{Concept Check 12:} Would you use the Binomial or Poisson distribution to describe the distribution of crossovers along a chromosome in meiosis? Justify your answer

\end{itemize}

\color{red} The Poisson distribution because, in general, there is a small number of crossovers compared to the relatively large number of possible crossovers given the entire length of the chromosomes. 

\color{black}

\bigskip

\subsection{Geometric Random Variable}

\begin{itemize}

\item The geometric random variable can take on values 1, 2, \ldots, $\infty$ describes the time until the first success in a sequence of independent trials with the probability of success, \textit{p}, and the probability of failure, \textit{q = 1-p}. 

\item Prob \{\textbf{X} = \textit{i}\} = \textit{$q^{i-1}$}\textit{p}

\item \textbf{Concept Check 13:} Why would you use the geometric distribution instead of the binomial distribution? (Hint: How are the questions answered by each different?) 

\end{itemize}

\color{red} The geometric distribution is used to get the probability of a given number of failures until a success. This means that as soon as there is a success, we stop recording trials. Alternatively, the binomial distribution allows us to have more than one success and the success(es) can be scattered throughout the independent trials rather than at the end. 

\color{black}

\bigskip

\section{Correlated random variables}

\begin{itemize}

\item \textbf{Concept Check 14:} If Prob\{\textbf{X} = \textit{$x_{i}$}\} = $p_{i}$ and Prob\{\textbf{Y} = $x_{i}$\} = \textit{$p_{j}$}, then the random variables \textit{X} and \textit{Y} are independent if \textit{$p_{ij}$} = \color{red} \textit{$p_{i}$} $\times$ \textit{$p_{j}$}

\color{black}

\item Covariance is defined as Cov \{\textbf{X,Y}\} =  \textit{E}\{\big($\textbf{X - $\mu_{x}$}$\big) \big($\textbf{Y - $\mu_{y}$}$\big)\}

\item The covariance is a measure of the tendency of two random variables to vary together. 

\bigskip
\item \textbf{Concept Check 15:} If \textit{X} and \textit{Y} tend to be large together and small together, then their covariance will be \color{red} positive

\color{black}

\item If two random variables are independent, then their covariance is \color{red} 0 \color{black}

\item The correlation coefficient of \textit{X} and \textit{Y} = \[\frac{Cov(X, Y)}{\sqrt(Var (X))\sqrt(Var (Y))} \] 

\bigskip
\item \textbf{Concept Check 16:} In the definition of the correlation coefficient, the covariance is scaled by the variance. Why is this ``normalization'' step important? What additional information can we get from this step? \color{red} We are comparing the relative size of the covariance to the variances. If the covariance is big, we do not know if this is due to a truly strong correlation between \textit{X} and \textit{Y} or if it is being driven by large variances of  \textit{X} and \textit{Y}. The correlation coefficient helps us to answer this question. Additionally, after normalization, we can compare correlation coefficients across different studies. While before the covariance may be big, this can be due to 

\color{black}

\bigskip
\item \textbf{Challenge 2:} Suppose we have two sites on a chromosome, \textit{A} and \textit{B}, each with two alleles in a haploid individual. If we were to plug in the probability of \textit{$A_{1}$} and \textit{$B_{1}$} into the formula for correlation coefficient, describe the information in words that we will get from this.

\color{red} This question introduces the concept of linkage disequilibrium. This means that we are looking for how often the alleles \textit{$A_{1}$} and \textit{$B_{1}$} ``travel together'' a.k.a. how often they appear together compared to what would be expected by chance. 
\color{black}

\bigskip
\textit{Note: These questions are based on material found in the Human Variation and Disease lectures titled ``LD'' and ``Quantative Genetics I''.}


 \end{itemize}


\bigskip
\section{Operations on random variables}

\begin{itemize}

\item If \textbf{Y} is a transformed random variable, \textbf{Y} = a\textbf{X} + b, \\ 
then \textit{E}\{\textbf{Y}\} = \textit{aE}\{\textbf{X}\} + \textit{b} and the Var\{\textbf{Y}\} = $a^{2}$Var\{\textbf{Y}\}.

\item  \textit{E}\{\textbf{X + Y}\} = \textit{E}\{\textbf{X}\} + \textit{E}\{\textbf{Y}\} regardless of whether \textit{X} and \textit{Y} and independent or dependent. \\

\item Var\{\textbf{X + Y}\} = Var\{\textbf{X}\} + Var\{\textbf{Y}\} + 2Cov\{\textbf{X,Y}\}. \\

\item \textbf{Concept Check 17:} As we can see, the variance of \{\textbf{X + Y}\} relies on the variance of \{\textbf{X}\}, the variance of \{\textbf{Y}\}, and the covariance of \{\textbf{X,Y}\}. Why do you think that is? 


\color{red} This implies that the variance of the mean increases with the average of the correlations. In other words, additional correlated observations are not as effective as additional independent observations at reducing the uncertainty of the mean. (Answer from the Wikipedia article titled ``Variance''.)

\color{black}

\bigskip
\item \textbf{Concept Check 18:}  What is the Var\{\textbf{X + Y}\} when \textbf{X} and \textbf{Y} are independent? \color{red} Var\{\textbf{X}\} + Var\{\textbf{Y}\} because 2Cov\{\textbf{X,Y}\} = 0. \color{black}

\end{itemize}

\bigskip

\bigskip

\bigskip
\section{Noteworthy continuous random variables}

\subsection{Overview}

\begin{itemize}

\item \textbf{Concept Check 19:}  Continuous random variables can have values from -$\infty$ to $\infty$. Therefore, the probability that a continuous random variable takes on 1 particular value is \color{red} negligible (0) \color{black}

\item As a result, we will write the probability of a continuous random variable in a particular interval:

\bigskip
Prob \{\textit{a} \textless \textbf{X} \textless \textit{b} \} = $\int_{a}^{b}$ \textit{f\big(x\big) dx} \\

where \textit{f\big(x\big)} is the probability density function. 

\item \textbf{Concept Check 20:} Prob \{\textit{- $\infty$} \textless \textbf{X} \textless \textit{$\infty$} \} = $\int_{- \infty}^{\infty}$ \textit{f\big(x\big) dx} equals \color{red} 1 

\color{black}

\end{itemize}

\subsection{Normal random variable}

\begin{itemize}

\item  \textbf{Concept Check 21:} How can you tell if a dataset is well described by the normal distribution? 

\color{red} We want to use the 68-95-99.7 rule for normal distributions. This means that we should check if approx. 68\% of the data falls within 1 standard deviation of the mean, 95\% fall within 2 standard deviations of the mean and 99.7\% fall within 3 standard deviations of the mean. 
\color{black}
\bigskip

\item \textbf{Concept Check 22:} List some phenotypes that we might model as normally distributed:

\color{red} Human height, blood pressure

\color{black}

\item The probability density function of the normal distribution is 
\begin{eqnarray}
\textit{f\big(x\big)} = \frac{1}{\sqrt{(2\pi)\sigma^{2}}} exp(-\frac{(x_{i}-\mu)^{2}}{2\sigma^{2}} )
\end{eqnarray}

\bigskip
\item We will often standardize a normal distribution, e.g. N(3, 10), using the transformation Z = 
$\frac{\textit{X} - \mu}{\sigma}$


\item  \textbf{Concept Check 23:} What are the advantages of ``standardizing'' a normal distribution with parameters $\mu$ and $\sigma^{2}$ so that it is N(0,1)?

\end{itemize}

\color{red} The biggest advantage is the ability to compare across studies, particularly if the original data in each of the studies are modeled by normal distributions with different means and variances. 

\color{black}

\subsection{Beta random variable} 

\begin{itemize}

\item The Beta distribution represents a distribution of probabilities. Therefore, 0 \textless  x \textless 1.

\item If \textit{n} is a positive integer, $\Gamma$\big(\textit{n}\big) = \big(\textit{n} - 1\big)!

\item The beta random variable is distributed according to the density:

 \begin{equation} 
\textit{f\big(x\big)} = \frac{\Gamma(\alpha + \beta)x^{\alpha-1}(1-x)^{\beta-1}}{\Gamma(\alpha)\Gamma(\beta) }
\end{equation}

\includegraphics[width =1\textwidth]{/Users/laurenblake/Dropbox/Human_Variation_and_Disease_TA/Beta.png}

\item  \textbf{Concept Check 24:} Above are PDFs of beta distributions with different alpha and beta values. You will see that on the x-axis are probabilities. How could having a distribution of probabilities be useful in a genetics context?

\color{red} We frequently are unsure of the true population probability and we want to find the best estimates of probabilities (e.g. allele frequencies, disease prevalence, etc.)

\color{black}

\end{itemize}

\subsection{Bivariate normal random variable (Challenging)}

\begin{itemize}

\item We can represent two correlated random variables that each follow a normal distribution with the bivariate normal distribution. We can represent these correlated variables with random vector \textit{Z}. This vector \textit{Z} will have the vector mean $\mu$ and covarariance matrix $\Sigma$.

\item The probability density for the bivariate normal distribution Z {\raise.17ex\hbox{$\scriptstyle\sim$}} \big(X,Y\big) incorporates the means of each of the random variables (\textit{X} and \textit{Y}), the variances of the two random variables, and the correlation coefficient, $\rho$.

\[
f(x,y)=\frac{\exp \left\{ -\frac 1{2(1-\rho ^2)}\left[ \left( \frac{x-\mu _x%
}{\sigma _x}\right) ^2-2\rho \left( \frac{x-\mu _x}{\sigma _x}\right) \left( 
\frac{y-\mu _y}{\sigma _y}\right) +\left( \frac{y-\mu _y}{\sigma _y}\right)
^2\right] \right\} }{2\pi \sigma _x\sigma _y\sqrt{1-\rho ^2}} 
\]

where $(\mu _x,\mu _y)$ is the mean vector and the variance-covariance
matrix is

\[
\left( 
\begin{array}{cc}
Var(X) & Cov(X,Y) \\ 
Cov(X,Y) & Var(Y)
\end{array}
\right) =\left( 
\begin{array}{cc}
\sigma _x^2 & \rho \sigma _x\sigma _y \\ 
\rho \sigma _x\sigma _y & \sigma _y^2
\end{array}
\right) 
\]

\bigskip
\item The bivariate normal can be generalized to include more than two univariate random variables, called multivariate normal distributions. 

\item A handful of the journal articles that we read in this class will use the bivariate or multivariate normal distribution (e.g. Pritchard et al. 2000 Genetics 155:945). 

\item We will also use the moments and properties of the bivariate normal distribution, for example, in linear regression. 

\item \textbf{Concept Check 25:}  What is the goal of linear regression? \color{red} To determine if variable X is a good predictor of variable Y. (Here, a ``good predictor'' is defined by being better than by chance. Also, there must be a linear relationship between X and Y.) 
\color{black}

\item The regression of \textbf{Y} on \textbf{X} is 


 \textit{E} \{\textbf{Y} $\mid$ \textbf{X} = x \} = $\mu_{y}$ + $\beta$ \big( \textit{x - $\mu_{x}$} \big)
 
\bigskip
where 

$\beta$ =  $\frac{Cov \{ \textbf{X},\textbf{Y} \} }{ \sigma_{x}^{2}} $

\bigskip
\textbf{Concept Check 26:} Use concepts that we have covered in this discussion and what you know about linear regression to write each of the parts of this equation in words and how it is related to linear regression. 

\bigskip  

\textit{E} \{\textbf{Y} $\mid$ \textbf{X} = x \} is 

\color{red} 

The expectation of Y given a particular value of X. This coincides nicely with the goal of regression because we want to be able to give our model a value of X and use that to come up with our best prediction of Y. 

\color{black}

$\mu_{y}$ is

\color{red} The average value of y. This is used because the mean is the best statistic to describe a variable in linear regression. \color{black}

$\beta$ is

\color{red} This is how strong the relationship between X and Y is relative to the variance (spread) of X. In genetics, this is often referred to as the ``effect size''. 

\color{black}

 \textit{x - $\mu_{x}$} is
 
\color{red} This is the difference between the random variable \textit{X} and the mean of X. This is useful because when you know how far the given value of X deviates from the mean so that you can use this information to figure out how the given value of Y deviates from the mean. 

\color{black}
 


\end{itemize}


\bigskip
\textbf{Note:} This worksheet was created by Lauren E. Blake using the material from ``Population Genetics, A Concise Guide'' by John Gillespie 2nd edition and ``Introduction to Probability'' by D. P. Bertsekas and J. N. Tsitsiklis, 1st edition. Kindly edited by course instructor John Novembre. 




\end{document}